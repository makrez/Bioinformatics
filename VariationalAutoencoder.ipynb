{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM65IkpBbq+DFeIUDp6UCBJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/makrez/BioinformaticsTools/blob/master/VariationalAutoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "!pip install biopython\n",
        "from Bio import SeqIO\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "!pip install umap-learn\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.decomposition import PCA\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot\n",
        "\n"
      ],
      "metadata": {
        "id": "cVgElsnRNdho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b16b5b-54c6-4ff4-c588-9bcfbb3a2d8d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.81)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.10.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.56.4)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.5.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.65.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn) (67.7.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.0.0+cu118)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UW8mo8RINC6D",
        "outputId": "c655206e-c341-4d3d-8f7d-1aa11ff23df8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class hot_dna:\n",
        "    def __init__(self, sequence):\n",
        "        sequence = sequence.upper()\n",
        "        self.sequence = self._preprocess_sequence(sequence)\n",
        "        self.category_mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3, 'U': 3, '-': 4, 'N': 5}\n",
        "        self.onehot = self._onehot_encode(self.sequence)\n",
        "\n",
        "    def _preprocess_sequence(self, sequence):\n",
        "        ambiguous_bases = {'R', 'Y', 'S', 'W', 'K', 'M', 'B', 'D', 'H', 'V'}\n",
        "        new_sequence = \"\"\n",
        "        for base in sequence:\n",
        "            if base in ambiguous_bases:\n",
        "                new_sequence += 'N'\n",
        "            else:\n",
        "                new_sequence += base\n",
        "        return new_sequence\n",
        "\n",
        "    def _onehot_encode(self, sequence):\n",
        "        integer_encoded = np.array([self.category_mapping[char] for char in sequence]).reshape(-1, 1)\n",
        "        onehot_encoder = OneHotEncoder(sparse=False, categories='auto', handle_unknown='ignore')\n",
        "        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "        \n",
        "        full_onehot_encoded = np.zeros((len(sequence), 6))\n",
        "        full_onehot_encoded[:, :onehot_encoded.shape[1]] = onehot_encoded\n",
        "        \n",
        "        return full_onehot_encoded\n",
        "\n",
        "flatted_sequence = list()\n",
        "sequence_labels = list()\n",
        "\n",
        "alignment_length = 4000\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/autoencoder_data/bacillus.aln') as handle:\n",
        "    for record in SeqIO.parse(handle, 'fasta'):\n",
        "        label = str(record.description).rsplit(';', 1)[-1]\n",
        "        seq_hot = hot_dna(str(record.seq)[10:alignment_length+10]).onehot\n",
        "        \n",
        "        if len(seq_hot) == alignment_length:\n",
        "            flatted_sequence.append(seq_hot)\n",
        "            sequence_labels.append(label)\n"
      ],
      "metadata": {
        "id": "qP74Bl0RNsum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, alignment_length=4000, latent_dim=50, fc_hidden=512):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(6, 12, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(12, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(alignment_length*32//8, fc_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc_hidden, fc_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc_hidden, fc_hidden),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.fc_mu = nn.Linear(fc_hidden, latent_dim)\n",
        "        self.fc_var = nn.Linear(fc_hidden, latent_dim)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, fc_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc_hidden, fc_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc_hidden, fc_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc_hidden, ((alignment_length//2)//2)*32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "            nn.ConvTranspose1d(32, 12, kernel_size=4, stride=2, padding=1), \n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(12, 6, kernel_size=4, stride=2, padding=1),\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, log_var = self.fc_mu(h), self.fc_var(h)\n",
        "        return mu, log_var\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = self.decoder(z)\n",
        "        h = h.view(h.shape[0], 32, -1)\n",
        "        out = self.decoder_conv(h)\n",
        "        out = F.softmax(out, dim=1)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        return self.decode(z), mu, log_var\n",
        "\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, sequences):\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.sequences[index]\n",
        "\n",
        "# def vae_loss(x, x_recon, mu, log_var):\n",
        "#     BCE = F.binary_cross_entropy(x_recon, x, reduction='sum')\n",
        "#     KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "#     return BCE + KLD\n",
        "\n",
        "loss_function = torch.nn.CosineEmbeddingLoss(reduction='none')\n",
        "\n",
        "def vae_loss(x, x_recon, mu, log_var, target):\n",
        "    cos_loss = loss_function(x_recon.contiguous().view(x_recon.size(0), -1), x.contiguous().view(x.size(0), -1), target)\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    return cos_loss.mean() + KLD\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "def train_model(model, optimizer, train_dataloader, val_dataloader, device, n_epochs):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # for epoch in range(1, n_epochs + 1):\n",
        "    #     train_loss = 0.0\n",
        "    #     val_loss = 0.0\n",
        "\n",
        "    #     # Training loop\n",
        "    #     model.train()\n",
        "    #     for batch_idx, data in enumerate(train_dataloader):\n",
        "    #         # Transform the data tensor to have the shape (batch_size, n_channels, sequence_length)\n",
        "    #         data = data.permute(0, 2, 1)\n",
        "    #         data = data.float().to(device)\n",
        "\n",
        "    #         # Zero the gradients\n",
        "    #         optimizer.zero_grad()\n",
        "\n",
        "    #         # Forward pass\n",
        "    #         x_recon, mu, log_var = model(data)\n",
        "\n",
        "    #         # Compute the loss\n",
        "    #         loss = vae_loss(data, x_recon, mu, log_var)\n",
        "\n",
        "    #         # Backward pass and optimization\n",
        "    #         loss.backward()\n",
        "    #         optimizer.step()\n",
        "\n",
        "    #         # Update the training loss\n",
        "    #         train_loss += loss.item() * data.size(0)\n",
        "\n",
        "    for batch_idx, data in enumerate(train_dataloader):\n",
        "        # Transform the data tensor to have the shape (batch_size, n_channels, sequence_length)\n",
        "        data = data.permute(0, 2, 1)\n",
        "        data = data.float().to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        x_recon, mu, log_var = model(data)\n",
        "\n",
        "        # Define the target tensor for the cosine embedding loss\n",
        "        target = torch.ones(data.size(0)).to(device)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = vae_loss(data, x_recon, mu, log_var, target)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the training loss\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "        # Print the average training loss for the epoch\n",
        "        train_loss /= len(train_dataloader.dataset)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, data in enumerate(val_dataloader):\n",
        "              # Transform the data tensor to have the shape (batch_size, n_channels, sequence_length)\n",
        "              data = data.permute(0, 2, 1)\n",
        "              data = data.float().to(device)\n",
        "\n",
        "              # Forward pass\n",
        "              x_recon, mu, log_var = model(data)\n",
        "\n",
        "              # Compute the loss\n",
        "              loss = vae_loss(data, x_recon, mu, log_var)\n",
        "\n",
        "              # Update the validation loss\n",
        "              val_loss += loss.item() * data.size(0)\n",
        "\n",
        "        val_loss /= len(val_dataloader.dataset)\n",
        "\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, val_loss))\n",
        "        \n",
        "        # Append the training loss to the list\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "\n",
        "def plot_latent_space(model, device, flatted_sequence, sequence_labels, latent_space_dim, hyperparameters):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Create a list to store the latent vectors\n",
        "    latent_vectors = []\n",
        "    labels = []\n",
        "\n",
        "    # Iterate over the sequences in the dataset and obtain their latent vectors\n",
        "    for sequence, label in zip(flatted_sequence, sequence_labels):\n",
        "        # Transform the sequence tensor to have the shape (1, n_channels, sequence_length)\n",
        "        sequence = torch.from_numpy(sequence).unsqueeze(0).permute(0, 2, 1)\n",
        "        sequence = sequence.float().to(device)\n",
        "\n",
        "        # Obtain the latent vector (mu)\n",
        "        mu, _ = model.encode(sequence)\n",
        "        mu = mu.detach().cpu().numpy()\n",
        "\n",
        "        # Add the latent vector to the list\n",
        "        latent_vectors.append(mu)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Convert the list of latent vectors to a 2D numpy array\n",
        "    latent_vectors_array = np.array(latent_vectors).squeeze()\n",
        "\n",
        "    if latent_space_dim > 2:\n",
        "        umap_model = umap.UMAP(n_neighbors=100, min_dist=0.1, random_state=42)\n",
        "        coords = umap_model.fit_transform(latent_vectors_array)\n",
        "    else:\n",
        "        coords = latent_vectors_array\n",
        "\n",
        "    # Create a scatter plot for each unique label\n",
        "    unique_labels = set(labels)\n",
        "    color_dict = {label: plt.cm.tab20(i) for i, label in enumerate(unique_labels)}\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    for label in unique_labels:\n",
        "        # Get the indices of the data points with this label\n",
        "        indices = [i for i, x in enumerate(labels) if x == label]\n",
        "\n",
        "        # Get the corresponding latent vectors or UMAP coordinates and colors\n",
        "        coords_subset = coords[indices]\n",
        "        color = color_dict[label]\n",
        "\n",
        "        # Add the scatter plot for this label to the axes\n",
        "        ax.scatter(coords_subset[:, 0], coords_subset[:, 1], color=color, label=label)\n",
        "\n",
        "    # Add the legend to the axes\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    ax.set_title(f\"Latent Dim: {hyperparameters['latent_dim']} | Learning Rate: {hyperparameters['learning_rate']} | Epochs: {hyperparameters['n_epochs']}\")\n",
        "\n",
        "    # Save the plot in the 'plots' subdirectory\n",
        "    os.makedirs(os.path.join(os.getcwd(), \"plots\"), exist_ok=True)\n",
        "    filename = f\"latent_dim_{hyperparameters['latent_dim']}_lr_{hyperparameters['learning_rate']}_n_epochs_{hyperparameters['n_epochs']}.png\"\n",
        "    plt.savefig(os.path.join(os.getcwd() , \"plots\", filename), bbox_inches=\"tight\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_loss(train_losses, val_losses, hyperparameters):\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses, label=\"Training Loss\")\n",
        "    plt.plot(val_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(f\"Latent Dim: {hyperparameters['latent_dim']} | Learning Rate: {hyperparameters['learning_rate']} | Epochs: {hyperparameters['n_epochs']}\")\n",
        "    plt.legend()\n",
        "    os.makedirs(os.path.join(os.getcwd(), \"plots\"), exist_ok=True)\n",
        "    filename = f\"loss_latent_dim_{hyperparameters['latent_dim']}_lr_{hyperparameters['learning_rate']}_n_epochs_{hyperparameters['n_epochs']}.png\"\n",
        "    plt.savefig(os.path.join(os.getcwd() , \"plots\", filename), bbox_inches=\"tight\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def main(alignment_length, latent_dim, learning_rate, n_epochs): #, architecture_name):\n",
        "    # Instantiate the model with the chosen architecture\n",
        "    model = VAE(alignment_length=alignment_length, latent_dim=latent_dim) #, architecture=architectures[architecture_name])\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Define the optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Define the dataloader\n",
        "    dataset = SequenceDataset(flatted_sequence)\n",
        "    \n",
        "    # Split the dataset into training and validation sets\n",
        "    train_set, val_set = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "    \n",
        "    train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Train the model\n",
        "    train_losses, val_losses = train_model(model, optimizer, train_dataloader, val_dataloader, device, n_epochs)\n",
        "\n",
        "    # Define the hyperparameters dictionary\n",
        "    hyperparameters = {\n",
        "        'latent_dim': latent_dim,\n",
        "        'learning_rate': learning_rate,\n",
        "        'n_epochs': n_epochs\n",
        "    }\n",
        "\n",
        "    # Plot the loss curves\n",
        "    plot_loss(train_losses, val_losses, hyperparameters)\n",
        "    \n",
        "    # Plot the latent space\n",
        "    plot_latent_space(model, device, flatted_sequence, sequence_labels, latent_dim, hyperparameters)\n",
        "    \n",
        "    # Calculate the latent vectors for the PCA plot\n",
        "    latent_vectors_array, _ = encode_latent_vectors(model, device, flatted_sequence)\n",
        "\n",
        "    # Plot the PCA\n",
        "    try:\n",
        "        plot_pca(latent_vectors_array.cpu().numpy(), sequence_labels, hyperparameters)\n",
        "    except Exception as e:\n",
        "        print(\"Failed to plot PCA:\", e)\n",
        "\n",
        "    # Plot the model\n",
        "    try:\n",
        "        plot_model(model)\n",
        "    except Exception as e:\n",
        "        print(\"Failed to plot model:\", e)\n",
        "\n",
        "def encode_latent_vectors(model, device, sequences):\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    with torch.no_grad(): # Deactivate gradients for the following code\n",
        "        sequences_tensor = torch.Tensor(sequences).to(device)\n",
        "        latent_vectors, _ = model.encode(sequences_tensor)\n",
        "    return latent_vectors\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    print(f\"alignment length is {alignment_length}\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    n_epochs = 10\n",
        "\n",
        "    latent_dims = [2,4]\n",
        "    learning_rates = [0.01]\n",
        "\n",
        "    for latent_dim in latent_dims:\n",
        "        for learning_rate in learning_rates:\n",
        "              main(alignment_length, latent_dim, learning_rate, n_epochs)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "pMiQKSq-GqfJ",
        "outputId": "344e3e92-537f-4e28-d157-2851ebc6f8b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alignment length is 4000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-545ede07b7e2>\u001b[0m in \u001b[0;36m<cell line: 305>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlatent_dim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlatent_dims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m               \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malignment_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-545ede07b7e2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(alignment_length, latent_dim, learning_rate, n_epochs)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;31m# Define the hyperparameters dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-545ede07b7e2>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, train_dataloader, val_dataloader, device, n_epochs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Update the training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;31m# Print the average training loss for the epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'train_loss' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F9pSd4GhQNlV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
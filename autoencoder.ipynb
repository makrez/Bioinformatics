{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDDbqWDSnhPpitNrLURPHM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/makrez/BioinformaticsTools/blob/master/autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln6lBweeRR5t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoders for the classification of DNA sequences\n",
        "\n",
        "### Loading libraries"
      ],
      "metadata": {
        "id": "3Eb9LBS0Ra1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import re\n",
        "!pip install biopython\n",
        "#import biopython"
      ],
      "metadata": {
        "id": "0AgteJClQGU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5714a61-e4e4-4c6f-f627-2ca026176d95"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.8/dist-packages (1.80)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from biopython) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating functions"
      ],
      "metadata": {
        "id": "-aNnBy2xRlBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class hot_dna:\n",
        " def __init__(self,fasta):\n",
        "   \n",
        "  #check for and grab sequence name\n",
        "  if re.search(\">\",fasta):\n",
        "   name = re.split(\"\\n\",fasta)[0]\n",
        "   sequence = re.split(\"\\n\",fasta)[1]\n",
        "  else :\n",
        "   name = 'unknown_sequence'\n",
        "   sequence = fasta\n",
        "  \n",
        "  #get sequence into an array\n",
        "  seq_array = array(list(sequence))\n",
        "    \n",
        "  #integer encode the sequence\n",
        "  label_encoder = LabelEncoder()\n",
        "  integer_encoded_seq = label_encoder.fit_transform(seq_array)\n",
        "    \n",
        "  #one hot the sequence\n",
        "  onehot_encoder = OneHotEncoder(sparse=False)\n",
        "  #reshape because that's what OneHotEncoder likes\n",
        "  integer_encoded_seq = integer_encoded_seq.reshape(len(integer_encoded_seq), 1)\n",
        "  onehot_encoded_seq = onehot_encoder.fit_transform(integer_encoded_seq)\n",
        "  \n",
        "  #add the attributes to self \n",
        "  self.name = name\n",
        "  self.sequence = fasta\n",
        "  self.integer = integer_encoded_seq\n",
        "  self.onehot = onehot_encoded_seq\n",
        "\n",
        "\n",
        "# Flatten a list\n",
        "def flatten(l):\n",
        "    return [item for sublist in l for item in sublist]\n",
        "\n",
        "\n",
        "# Autoencoder\n",
        "\n",
        "class AE(tf.keras.Model):\n",
        "  def __init__(self, in_size, n_code, noise_rate=0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.data_size = list(in_size)  # shape of data sample\n",
        "    self.flat_data_size = np.prod(self.data_size)\n",
        "    \n",
        "    self.noise_rate = min(0.99, max(0, noise_rate)) # noise rate for denoising AE\n",
        "    self.denoising = self.noise_rate != 0\n",
        "\n",
        "    self.x_d = None  # variable to keep the input data\n",
        "    self.xn_d = None # input data with added noise (for DAE)\n",
        "\n",
        "    self.x_d_val = None  # validation dataset \n",
        "    self.c_d_val = None  # class labels for validation dataset\n",
        "\n",
        "    self.history = {} # training history\n",
        "    self.sample_history = {}  # history of validation sample evolution in latent space and reconstruction\n",
        "    self.weights_history = {} # history of model weights (joint model can't be saved at the moment)\n",
        "\n",
        "    self.out = display(IPython.display.Pretty(''), display_id=True)\n",
        "\n",
        "    self.last_n_ep = 0  # number of epochs of last fit run\n",
        "\n",
        "    self.n_code = n_code # number of latent dimensions\n",
        "\n",
        "    self.encoder = None\n",
        "    self.decoder = None\n",
        "    \n",
        "    self.create()\n",
        "\n",
        "  def create(self):\n",
        "    \"\"\"\n",
        "    Here the model is built\n",
        "    \"\"\"\n",
        "\n",
        "    # encoder model\n",
        "    self.encoder = tf.keras.Sequential(\n",
        "        [\n",
        "         Input(shape=self.data_size),\n",
        "         Flatten(),\n",
        "         Dense(128, activation='relu', kernel_initializer='he_normal', name='e_l1'),\n",
        "         Dense(self.n_code, activation='sigmoid', kernel_initializer='he_normal', name='e_l2'),\n",
        "        ])\n",
        "    \n",
        "    #decoder model\n",
        "    self.decoder = tf.keras.Sequential(\n",
        "        [\n",
        "         Input(shape=self.n_code),\n",
        "         Dense(128, activation='relu', kernel_initializer='he_normal', name='d_l1'),\n",
        "         Dense(self.flat_data_size, activation='sigmoid', kernel_initializer='he_normal', name='d_l2'),\n",
        "         Reshape(target_shape=self.data_size,),\n",
        "        ])\n",
        "         \n",
        "\n",
        "    # build the model\n",
        "    self.compile(\n",
        "          optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "          loss=tf.keras.losses.MeanSquaredError(),\n",
        "      )\n",
        "      \n",
        "  def encode(self, x):\n",
        "    z = self.encoder(x)\n",
        "    return z\n",
        "\n",
        "  def decode(self, z):\n",
        "    y = self.decoder(z)\n",
        "    return y\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"forward pass\"\"\"\n",
        "    z = self.encoder(x)\n",
        "    y = self.decoder(z)\n",
        "    return y\n",
        "\n",
        "  class EvalNSamples(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Callback class for evaluating ans saving latent state and prediciton for validation samples\n",
        "    \"\"\"\n",
        "    def __init__(self, ae, n=32):\n",
        "      super().__init__()\n",
        "      self.ae = ae\n",
        "      self.n_sampl = n\n",
        "\n",
        "      self.ims_smpl = None\n",
        "      self.lbls_smpl = None\n",
        "\n",
        "      self.get_uniform_subsample()\n",
        "\n",
        "    def get_uniform_subsample(self):\n",
        "      \"\"\"\n",
        "      Get self.n_sampl elements for each of the classes\n",
        "      for latent space evolution\n",
        "      \"\"\"\n",
        "      ims = []\n",
        "      lbls = []\n",
        "      for class_idx in range(np.max(self.ae.c_d_val)+1):\n",
        "        map_d = self.ae.c_d_val == class_idx\n",
        "        ims_d = self.ae.x_d_val[map_d]\n",
        "\n",
        "        smpl_idx = np.random.choice(len(ims_d), self.n_sampl)\n",
        "        ims_d_smpl = ims_d[smpl_idx]\n",
        "        \n",
        "        ims.append(ims_d_smpl)\n",
        "        lbls.append([class_idx]*self.n_sampl)\n",
        "\n",
        "      self.ims_smpl = np.concatenate(ims)\n",
        "      self.lbls_smpl = np.concatenate(lbls)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        samples = self.ae.x_d_val[:self.n_sampl]\n",
        "        labels = self.ae.c_d_val[:self.n_sampl]\n",
        "        res = {'x': samples, 'l': labels}\n",
        "        if self.ae.denoising:\n",
        "          samples = self.ae.add_noise(samples)\n",
        "          res['xn'] = samples\n",
        "\n",
        "        res['y'] = self.ae.predict(samples)\n",
        "        res['z'] = self.ae.encoder(samples).numpy()\n",
        "\n",
        "        res['l_unif'] = self.lbls_smpl\n",
        "        res['z_unif'] = self.ae.encoder(self.ims_smpl).numpy()\n",
        "\n",
        "        self.ae.sample_history[epoch] = res   \n",
        "        #keys = list(logs.keys())\n",
        "        #print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
        "\n",
        "  class SaveAE(tf.keras.callbacks.Callback):\n",
        "      \"\"\"\n",
        "      Callback class for saving model weights along training\n",
        "      \"\"\"\n",
        "      def __init__(self, ae):\n",
        "        super().__init__()\n",
        "        self.ae = ae\n",
        "\n",
        "      def on_epoch_end(self, epoch, logs=None):\n",
        "        weights_encoder = self.ae.encoder.get_weights()\n",
        "        weights_decoder = self.ae.decoder.get_weights()\n",
        "\n",
        "        self.ae.weights_history[epoch] = {\n",
        "            'w_encoder': weights_encoder,\n",
        "            'w_decoder': weights_decoder,\n",
        "        }   \n",
        "\n",
        "  def _fit(self, x, y=None, epochs=None, batch_size=None,\n",
        "           validation_data=None, callbacks=None):\n",
        "    \"\"\"\n",
        "    Here actual model fitting is performed.\n",
        "    Can be reimplemented in inherited class for custom training loop (needed for VAE)\n",
        "    \"\"\"\n",
        "    if y is None:\n",
        "          return super().fit(x=x,\n",
        "                       epochs=epochs,\n",
        "                       validation_data=validation_data,\n",
        "                       callbacks=callbacks)\n",
        "  \n",
        "    else:\n",
        "      return super().fit(x=x, y=y,\n",
        "                        epochs=epochs, batch_size=batch_size, \n",
        "                        validation_data=validation_data,\n",
        "                        callbacks=callbacks)\n",
        "  \n",
        "  def fit(self, training_data, n_epochs, \n",
        "          validation_data=None, lr=None, \n",
        "          batch_size = 64,\n",
        "          epoch_callback=None,\n",
        "          callbacks=None\n",
        "          ):\n",
        "    \"\"\"\n",
        "    Interface for model training\n",
        "    Incapsulates all the callbacks, adding noise to training data etc\n",
        "    \"\"\"\n",
        "\n",
        "    t0 = timer()\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(training_data)\n",
        "    train_dataset = train_dataset.map(lambda x: (x, self.add_noise(x)))\n",
        "    train_dataset = train_dataset.shuffle(60000)\n",
        "    train_dataset = train_dataset.batch(batch_size)\n",
        "    train_dataset = train_dataset.prefetch(5)\n",
        "\n",
        "    self.x_d = training_data\n",
        "    self.x_d_val, self.c_d_val = validation_data\n",
        "    \n",
        "    #self.xn_d = self.add_noise(self.x_d)\n",
        "\n",
        "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "    callbacks = callbacks or []\n",
        "    callbacks = [AE.EvalNSamples(self), AE.SaveAE(self), tensorboard_callback]\n",
        "    # if save_dir:\n",
        "    #   callbacks += [save_callback]\n",
        "    \n",
        "    if lr is not None:\n",
        "      self.optimizer.lr.assign(lr)\n",
        "\n",
        "    self.history = self._fit(train_dataset,\n",
        "                                  epochs=n_epochs, \n",
        "                                  validation_data=(self.x_d_val, \n",
        "                                                   self.x_d_val),\n",
        "                                  callbacks=callbacks)\n",
        "                \n",
        "    self.last_n_ep = n_epochs\n",
        "    t1 = timer()\n",
        "    self.print(f'fit time {t1-t0:.0f} sec')\n",
        "\n",
        "  def add_noise(self, x):\n",
        "    \"\"\"\n",
        "    Adds Salt&Pepper nois to imput data.\n",
        "    Currently noisy samples are generated only once, not for each epoch.\n",
        "    \"\"\"\n",
        "    \n",
        "    if self.denoising:\n",
        "      sh = x.shape \n",
        "      \n",
        "      noise_mask = np.random.binomial(n=1, p=self.noise_rate, size=sh)\n",
        "      sp_noise = np.random.binomial(n=1, p=0.5, size=sh)\n",
        "\n",
        "      x = x * (1-noise_mask) + sp_noise * noise_mask\n",
        "\n",
        "      self.sp = sp_noise\n",
        "\n",
        "    return x\n",
        "\n",
        "  def print(self, msg):\n",
        "    self.out.update(IPython.display.Pretty(msg))\n",
        "\n",
        "  def summary(self):\n",
        "    self.encoder.summary()\n",
        "    self.decoder.summary()\n",
        "\n",
        "  def plot_hist(self):\n",
        "    \"\"\"\n",
        "    plot training loss\n",
        "    \"\"\"\n",
        "    hist = self.history.history\n",
        "    if not hist:\n",
        "      self.print('run `fit` first to train the model')\n",
        "      return\n",
        "\n",
        "    loss = hist['loss']\n",
        "    v_loss = hist['val_loss']\n",
        "    eps = np.arange(len(loss))\n",
        "    plt.semilogy(eps, loss, label='training');\n",
        "    if 'val_loss' in hist:\n",
        "      plt.semilogy(eps, v_loss, label='validation');\n",
        "    plt.legend()\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "  def plot_samples(self, stride=5, fig_scale=1):\n",
        "    \"\"\"\n",
        "    Plots input, noisy samples (for DAE) and reconstruction.\n",
        "    Each `stride`-th epoch\n",
        "    \"\"\"\n",
        "\n",
        "    hist = self.sample_history\n",
        "    for epoch_idx, hist_el in hist.items():\n",
        "      if epoch_idx % stride != 0 and epoch_idx != np.max(list(hist.keys())):\n",
        "        continue\n",
        "        \n",
        "      samples = []\n",
        "      for k, els in hist_el.items():\n",
        "        if k not in ['x', 'xn', 'y']:\n",
        "          continue\n",
        "        samples.append(els)\n",
        "\n",
        "      ny = len(samples)\n",
        "      nx = len(samples[0])\n",
        "      plt.figure(figsize=(fig_scale*nx, fig_scale*ny))\n",
        "      m = mosaic(samples)\n",
        "      plt.title(f'after epoch {int(epoch_idx)}')\n",
        "      plt.imshow(m, cmap='gray', vmin=0, vmax=1)\n",
        "      plt.tight_layout(0.1, 0, 0)\n",
        "      plt.show()\n",
        "      plt.close()\n",
        "\n",
        "  def run_on_trained(self, run_fn, ep=None):\n",
        "    \"\"\"\n",
        "    Helper funcrion to excecute any function on model in state after `ep` training epoch\n",
        "    \"\"\"\n",
        "    ep = ep if (ep is not None) else (self.last_n_ep-1)\n",
        "    self.encoder.set_weights(self.weights_history[ep]['w_encoder'])\n",
        "    self.decoder.set_weights(self.weights_history[ep]['w_decoder'])\n",
        "    \n",
        "    run_fn(self)\n",
        "\n",
        "  def run_on_all_training_history(self, run_fn, n_ep=None):\n",
        "    \"\"\"\n",
        "    Helper funcrion to excecute any function on model state after each of the training epochs\n",
        "    \"\"\"\n",
        "    n_ep = n_ep if (n_ep is not None) else (self.last_n_ep)\n",
        "    for ep in range(n_ep):\n",
        "      self.print(f'running on epoch {ep+1}/{n_ep}...')\n",
        "      self.run_on_trained(run_fn, ep)\n",
        "    self.print(f'done')"
      ],
      "metadata": {
        "id": "4lDABeOXRqqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read data"
      ],
      "metadata": {
        "id": "OUPPHa3eSAgL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "styulmSWWv6U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}